#!/usr/bin/env python3

"""
(Part 2) Evaluate Predictions using aac-metrics.

This script runs in a separate 'af3-eval' conda environment.
It loads the JSON file generated by Part 1 and computes CIDEr-D and FENSE scores.
"""

import json
import argparse
import time
from typing import List, Dict, Any
import sys


def evaluate_metrics(data: List[Dict[str, Any]]) -> Dict[str, float]:
    """
    Evaluates generated captions using CIDEr-D and FENSE.

    Args:
        data: A list of dictionaries, each containing 'prediction' and 'references'.

    Returns:
        A dictionary containing the calculated scores for each metric.
    """
    try:
        from aac_metrics import evaluate
    except ImportError:
        print("Error: `aac-metrics` is not installed.", file=sys.stderr)
        print(
            "Please activate the 'af3-eval' environment and run `pip install aac-metrics`",
            file=sys.stderr,
        )
        sys.exit(1)

    # Prepare lists of candidates (predictions) and multi-references
    candidates = []
    mult_references = []

    for item in data:
        # Only evaluate samples that have a non-empty prediction
        if item.get("prediction", "").strip():
            candidates.append(item["prediction"].replace("\n", " "))
            mult_references.append(item["references"])

    if not candidates:
        print("Error: No valid predictions found in the input file.", file=sys.stderr)
        return {}

    print(f"Evaluating {len(candidates)} valid predictions with CIDEr-D and FENSE...")

    try:
        corpus_scores, _ = evaluate(
            candidates=candidates,
            mult_references=mult_references,
            metrics=["cider_d", "fense"],
            verbose=1,  # Show progress bars from aac-metrics
        )

        # Convert tensor values to standard Python floats
        scores_dict = {
            metric: float(value.item()) for metric, value in corpus_scores.items()
        }
        return scores_dict

    except Exception as e:
        print(f"An error occurred during metric calculation: {e}", file=sys.stderr)
        return {}


def main():
    parser = argparse.ArgumentParser(
        description="Part 2: Calculate metrics from a predictions file."
    )
    parser.add_argument(
        "--input_json_path",
        type=str,
        required=True,
        help="Path to the JSON file containing predictions and references.",
    )
    parser.add_argument(
        "--output_json_path",
        type=str,
        default="final_scores.json",
        help="Output file for final scores.",
    )

    args, _ = parser.parse_known_args()

    # 1. Load data from JSON file
    try:
        with open(args.input_json_path, "r") as f:
            evaluation_data = json.load(f)
    except FileNotFoundError:
        print(f"Error: Input file not found at {args.input_json_path}", file=sys.stderr)
        sys.exit(1)

    # 2. Calculate scores
    scores = evaluate_metrics(evaluation_data)

    # 3. Display and Save Results
    if scores:
        print("\n" + "=" * 50)
        print("EVALUATION RESULTS")
        print("=" * 50)
        print(f"  {'CIDEr-D':<10}: {scores.get('cider_d', 0.0):.4f}")
        print(f"  {'FENSE':<10}: {scores.get('fense', 0.0):.4f}")
        print("=" * 50)
    else:
        print("\nEvaluation failed or produced no scores.")
        sys.exit(1)

    final_results = {
        "dataset": "Clotho-v2",
        "evaluated_samples": len(
            [item for item in evaluation_data if item.get("prediction", "").strip()]
        ),
        "total_samples": len(evaluation_data),
        "scores": scores,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
    }

    with open(args.output_json_path, "w") as f:
        json.dump(final_results, f, indent=4)

    print(f"Final scores saved to {args.output_json_path}")


if __name__ == "__main__":
    main()
